"""
æ€§èƒ½åŸºå‡†æµ‹è¯•æ¡†æ¶
ç”¨äºç›‘æ§æ ¸å¿ƒåŠŸèƒ½çš„æ€§èƒ½è¡¨ç°ï¼Œç¡®ä¿é‡æ„è¿‡ç¨‹ä¸­æ€§èƒ½ä¸é€€åŒ–
"""

import pytest
import sys
import os
import time
import statistics
import json
from dataclasses import dataclass, asdict
from typing import Dict, List, Any, Optional
from pathlib import Path
import pandas as pd

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from tests.conftest import (
    TEST_CONFIG,
    benchmark,
    create_mock_archive_data,
    create_mock_template,
    MockHeightCalculator
)

@dataclass
class PerformanceMetric:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç»“æ„"""
    name: str
    value: float
    unit: str
    description: str
    timestamp: float
    
@dataclass
class BenchmarkResult:
    """åŸºå‡†æµ‹è¯•ç»“æœ"""
    test_name: str
    iterations: int
    min_time: float
    max_time: float
    avg_time: float
    median_time: float
    std_dev: float
    metrics: List[PerformanceMetric]
    metadata: Dict[str, Any]

class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•åŸºç¡€ç±»"""
    
    def __init__(self, name: str, iterations: int = None):
        self.name = name
        self.iterations = iterations or TEST_CONFIG['benchmark_iterations']
        self.results: List[BenchmarkResult] = []
        self.baseline_file = Path(__file__).parent / 'performance_baseline.json'
    
    def run_benchmark(self, func, *args, **kwargs) -> BenchmarkResult:
        """è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•"""
        times = []
        metrics = []
        
        print(f"\nğŸš€ è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•: {self.name}")
        print(f"ğŸ“Š è¿­ä»£æ¬¡æ•°: {self.iterations}")
        
        for i in range(self.iterations):
            print(f"â±ï¸  è¿­ä»£ {i+1}/{self.iterations}...", end=" ")
            
            start_time = time.perf_counter()
            result = func(*args, **kwargs)
            end_time = time.perf_counter()
            
            elapsed = end_time - start_time
            times.append(elapsed)
            
            print(f"{elapsed:.4f}s")
            
            # æ”¶é›†é¢å¤–æŒ‡æ ‡
            if isinstance(result, dict) and 'metrics' in result:
                metrics.extend(result['metrics'])
        
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        min_time = min(times)
        max_time = max(times)
        avg_time = statistics.mean(times)
        median_time = statistics.median(times)
        std_dev = statistics.stdev(times) if len(times) > 1 else 0.0
        
        # åˆ›å»ºç»“æœå¯¹è±¡
        benchmark_result = BenchmarkResult(
            test_name=self.name,
            iterations=self.iterations,
            min_time=min_time,
            max_time=max_time,
            avg_time=avg_time,
            median_time=median_time,
            std_dev=std_dev,
            metrics=metrics,
            metadata={
                'timestamp': time.time(),
                'python_version': sys.version,
                'test_config': TEST_CONFIG
            }
        )
        
        self.results.append(benchmark_result)
        self._print_results(benchmark_result)
        
        return benchmark_result
    
    def _print_results(self, result: BenchmarkResult):
        """æ‰“å°åŸºå‡†æµ‹è¯•ç»“æœ"""
        print(f"\nğŸ“ˆ æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœ - {result.test_name}")
        print("=" * 50)
        print(f"ğŸ”„ è¿­ä»£æ¬¡æ•°: {result.iterations}")
        print(f"âš¡ æœ€å¿«æ—¶é—´: {result.min_time:.4f}s")
        print(f"ğŸŒ æœ€æ…¢æ—¶é—´: {result.max_time:.4f}s")
        print(f"ğŸ“Š å¹³å‡æ—¶é—´: {result.avg_time:.4f}s")
        print(f"ğŸ“ ä¸­ä½æ—¶é—´: {result.median_time:.4f}s")
        print(f"ğŸ“ æ ‡å‡†å·®:   {result.std_dev:.4f}s")
        
        if result.metrics:
            print(f"\nğŸ“‹ é¢å¤–æŒ‡æ ‡:")
            for metric in result.metrics:
                print(f"  â€¢ {metric.name}: {metric.value:.4f} {metric.unit}")
        
        print("=" * 50)
    
    def save_baseline(self, filename: str = None):
        """ä¿å­˜æ€§èƒ½åŸºçº¿"""
        baseline_file = Path(filename) if filename else self.baseline_file
        
        baseline_data = {
            'timestamp': time.time(),
            'results': [asdict(result) for result in self.results]
        }
        
        baseline_file.parent.mkdir(exist_ok=True)
        with open(baseline_file, 'w', encoding='utf-8') as f:
            json.dump(baseline_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ æ€§èƒ½åŸºçº¿å·²ä¿å­˜åˆ°: {baseline_file}")
    
    def load_baseline(self, filename: str = None) -> Dict[str, Any]:
        """åŠ è½½æ€§èƒ½åŸºçº¿"""
        baseline_file = Path(filename) if filename else self.baseline_file
        
        if not baseline_file.exists():
            print(f"âš ï¸  åŸºçº¿æ–‡ä»¶ä¸å­˜åœ¨: {baseline_file}")
            return {}
        
        with open(baseline_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def compare_with_baseline(self, tolerance: float = 0.2) -> bool:
        """ä¸åŸºçº¿è¿›è¡Œæ€§èƒ½å¯¹æ¯”"""
        baseline_data = self.load_baseline()
        
        if not baseline_data or not self.results:
            print("âš ï¸  æ— æ³•è¿›è¡ŒåŸºçº¿å¯¹æ¯”ï¼šç¼ºå°‘åŸºçº¿æ•°æ®æˆ–å½“å‰ç»“æœ")
            return True
        
        print(f"\nğŸ” æ€§èƒ½å¯¹æ¯”åˆ†æ (å®¹å¿åº¦: Â±{tolerance*100:.1f}%)")
        print("=" * 60)
        
        baseline_results = {r['test_name']: r for r in baseline_data['results']}
        all_passed = True
        
        for current_result in self.results:
            test_name = current_result.test_name
            baseline_result = baseline_results.get(test_name)
            
            if not baseline_result:
                print(f"ğŸ†• æ–°æµ‹è¯•: {test_name} - {current_result.avg_time:.4f}s")
                continue
            
            baseline_avg = baseline_result['avg_time']
            current_avg = current_result.avg_time
            change_ratio = (current_avg - baseline_avg) / baseline_avg
            
            status = "âœ… é€šè¿‡"
            if abs(change_ratio) > tolerance:
                status = "âŒ æ€§èƒ½é€€åŒ–" if change_ratio > 0 else "ğŸš€ æ€§èƒ½æå‡"
                if change_ratio > tolerance:
                    all_passed = False
            
            print(f"{status} {test_name}:")
            print(f"  åŸºçº¿: {baseline_avg:.4f}s")
            print(f"  å½“å‰: {current_avg:.4f}s")
            print(f"  å˜åŒ–: {change_ratio:+.2%}")
            print()
        
        return all_passed

class TestHeightCalculationPerformance:
    """è¡Œé«˜è®¡ç®—æ€§èƒ½æµ‹è¯•"""
    
    def test_height_calculation_benchmark(self):
        """è¡Œé«˜è®¡ç®—æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        benchmark = PerformanceBenchmark("height_calculation", iterations=50)
        
        def height_calc_test():
            calculator = MockHeightCalculator()
            
            # æµ‹è¯•ä¸åŒå¤æ‚åº¦çš„æ–‡æœ¬
            test_cases = [
                ("çŸ­æ–‡æœ¬", 20.0),
                ("ä¸­ç­‰é•¿åº¦çš„æ–‡æœ¬å†…å®¹ï¼ŒåŒ…å«ä¸­è‹±æ–‡æ··åˆå­—ç¬¦", 15.0),
                ("è¿™æ˜¯ä¸€ä¸ªéå¸¸é•¿çš„æ–‡æœ¬å†…å®¹ï¼ŒåŒ…å«å¤§é‡çš„ä¸­æ–‡å­—ç¬¦å’Œè‹±æ–‡å­—ç¬¦ï¼Œç”¨äºæµ‹è¯•åœ¨è¾ƒé•¿æ–‡æœ¬æƒ…å†µä¸‹çš„è¡Œé«˜è®¡ç®—æ€§èƒ½è¡¨ç°ã€‚" * 3, 10.0)
            ]
            
            start_time = time.perf_counter()
            calculations = 0
            
            for text, width in test_cases:
                from tests.conftest import create_mock_xlwings_range
                mock_range = create_mock_xlwings_range()
                
                # æµ‹è¯•ä¸‰ç§è®¡ç®—æ–¹æ³•
                for method in ['xlwings', 'gdi', 'pillow']:
                    calculator.set_method(method)
                    height = calculator.calculate_height(mock_range, text, width)
                    calculations += 1
            
            total_time = time.perf_counter() - start_time
            
            stats = calculator.get_performance_stats()
            
            return {
                'calculations': calculations,
                'total_time': total_time,
                'metrics': [
                    PerformanceMetric(
                        name=f"{method}_avg_time",
                        value=data['avg_time'],
                        unit="s",
                        description=f"{method}æ–¹æ³•å¹³å‡è®¡ç®—æ—¶é—´",
                        timestamp=time.time()
                    )
                    for method, data in stats.items() if data['count'] > 0
                ]
            }
        
        result = benchmark.run_benchmark(height_calc_test)
        assert result.avg_time < 1.0  # åº”è¯¥åœ¨1ç§’å†…å®Œæˆ
        return result

class TestDataProcessingPerformance:
    """æ•°æ®å¤„ç†æ€§èƒ½æµ‹è¯•"""
    
    def test_large_dataset_processing(self, test_env):
        """å¤§æ•°æ®é›†å¤„ç†æ€§èƒ½æµ‹è¯•"""
        benchmark = PerformanceBenchmark("large_dataset_processing", iterations=5)
        
        def data_processing_test():
            # åˆ›å»ºå¤§å‹æ•°æ®é›†
            large_data = create_mock_archive_data(1000)  # 1000æ¡è®°å½•
            
            start_time = time.perf_counter()
            
            # æ¨¡æ‹Ÿæ•°æ®å¤„ç†æ“ä½œ
            processed_groups = {}
            for _, row in large_data.iterrows():
                archive_id = row['æ¡ˆå·æ¡£å·']
                if archive_id not in processed_groups:
                    processed_groups[archive_id] = []
                processed_groups[archive_id].append(row)
            
            # æ¨¡æ‹Ÿæ•°æ®è¿‡æ»¤å’Œæ’åº
            filtered_data = large_data[large_data['é¡µæ•°'].astype(int) > 0]
            sorted_data = filtered_data.sort_values('æ¡ˆå·æ¡£å·')
            
            processing_time = time.perf_counter() - start_time
            
            return {
                'processed_records': len(large_data),
                'unique_archives': len(processed_groups),
                'filtered_records': len(filtered_data),
                'processing_time': processing_time,
                'metrics': [
                    PerformanceMetric(
                        name="records_per_second",
                        value=len(large_data) / processing_time,
                        unit="records/s",
                        description="æ¯ç§’å¤„ç†è®°å½•æ•°",
                        timestamp=time.time()
                    )
                ]
            }
        
        result = benchmark.run_benchmark(data_processing_test)
        
        # éªŒè¯å¤„ç†é€Ÿåº¦
        records_per_second = 1000 / result.avg_time
        assert records_per_second > 100  # æ¯ç§’è‡³å°‘å¤„ç†100æ¡è®°å½•
        
        return result
    
    def test_excel_file_operations(self, test_env):
        """Excelæ–‡ä»¶æ“ä½œæ€§èƒ½æµ‹è¯•"""
        benchmark = PerformanceBenchmark("excel_file_operations", iterations=10)
        
        def excel_ops_test():
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            test_data = create_mock_archive_data(100)
            
            start_time = time.perf_counter()
            
            # Excelå†™å…¥æ“ä½œ
            excel_path = os.path.join(test_env.temp_dir, 'perf_test.xlsx')
            test_data.to_excel(excel_path, index=False)
            
            # Excelè¯»å–æ“ä½œ
            loaded_data = pd.read_excel(excel_path)
            
            # æ•°æ®éªŒè¯
            assert len(loaded_data) == len(test_data)
            assert list(loaded_data.columns) == list(test_data.columns)
            
            file_ops_time = time.perf_counter() - start_time
            
            # è·å–æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(excel_path)
            
            return {
                'file_size': file_size,
                'records': len(test_data),
                'ops_time': file_ops_time,
                'metrics': [
                    PerformanceMetric(
                        name="file_size",
                        value=file_size / 1024,
                        unit="KB",
                        description="ç”Ÿæˆæ–‡ä»¶å¤§å°",
                        timestamp=time.time()
                    ),
                    PerformanceMetric(
                        name="throughput",
                        value=len(test_data) / file_ops_time,
                        unit="records/s",
                        description="æ–‡ä»¶æ“ä½œååé‡",
                        timestamp=time.time()
                    )
                ]
            }
        
        result = benchmark.run_benchmark(excel_ops_test)
        assert result.avg_time < 5.0  # Excelæ“ä½œåº”è¯¥åœ¨5ç§’å†…å®Œæˆ
        return result

class TestMemoryUsagePerformance:
    """å†…å­˜ä½¿ç”¨æ€§èƒ½æµ‹è¯•"""
    
    def test_memory_efficiency(self):
        """å†…å­˜æ•ˆç‡æµ‹è¯•"""
        import psutil
        import gc
        
        benchmark = PerformanceBenchmark("memory_efficiency", iterations=3)
        
        def memory_test():
            process = psutil.Process()
            
            # è®°å½•åˆå§‹å†…å­˜
            gc.collect()  # å¼ºåˆ¶åƒåœ¾å›æ”¶
            initial_memory = process.memory_info().rss / 1024 / 1024  # MB
            
            # åˆ›å»ºå¤§é‡æ•°æ®
            large_datasets = []
            for i in range(10):
                data = create_mock_archive_data(500)
                large_datasets.append(data)
            
            # è®°å½•å³°å€¼å†…å­˜
            peak_memory = process.memory_info().rss / 1024 / 1024  # MB
            
            # æ¸…ç†æ•°æ®
            del large_datasets
            gc.collect()
            
            # è®°å½•æ¸…ç†åå†…å­˜
            final_memory = process.memory_info().rss / 1024 / 1024  # MB
            
            memory_used = peak_memory - initial_memory
            memory_leaked = final_memory - initial_memory
            
            return {
                'initial_memory': initial_memory,
                'peak_memory': peak_memory,
                'final_memory': final_memory,
                'memory_used': memory_used,
                'memory_leaked': memory_leaked,
                'metrics': [
                    PerformanceMetric(
                        name="memory_usage",
                        value=memory_used,
                        unit="MB",
                        description="å†…å­˜ä½¿ç”¨é‡",
                        timestamp=time.time()
                    ),
                    PerformanceMetric(
                        name="memory_leak",
                        value=memory_leaked,
                        unit="MB",
                        description="å†…å­˜æ³„æ¼é‡",
                        timestamp=time.time()
                    )
                ]
            }
        
        result = benchmark.run_benchmark(memory_test)
        
        # å†…å­˜æ³„æ¼åº”è¯¥å¾ˆå°‘
        assert all(m.value < 50 for m in result.metrics if m.name == "memory_leak")
        
        return result

class TestConfigurationPerformance:
    """é…ç½®æ“ä½œæ€§èƒ½æµ‹è¯•"""
    
    def test_config_operations_benchmark(self, test_env):
        """é…ç½®æ“ä½œæ€§èƒ½åŸºå‡†æµ‹è¯•"""
        benchmark = PerformanceBenchmark("config_operations", iterations=20)
        
        def config_ops_test():
            from utils.config_manager import ConfigManager
            
            config_file = os.path.join(test_env.temp_dir, 'perf_config.json')
            manager = ConfigManager(config_file)
            
            start_time = time.perf_counter()
            
            # æ‰¹é‡é…ç½®æ“ä½œ
            operations = 0
            
            # è®¾ç½®æ“ä½œ
            for i in range(100):
                manager.set_path('template_path', f'/test/template_{i}.xlsx')
                manager.set_option('start_file', f'FILE_{i:03d}')
                manager.set_last_recipe('æ¡ˆå·ç›®å½•')
                operations += 3
            
            # ä¿å­˜æ“ä½œ
            manager.save_config()
            operations += 1
            
            # è¯»å–æ“ä½œ
            for i in range(100):
                template_path = manager.get('paths.template_path')
                start_file = manager.get('options.start_file')
                recipe = manager.get_last_recipe()
                operations += 3
            
            ops_time = time.perf_counter() - start_time
            
            return {
                'operations': operations,
                'ops_time': ops_time,
                'metrics': [
                    PerformanceMetric(
                        name="ops_per_second",
                        value=operations / ops_time,
                        unit="ops/s",
                        description="æ¯ç§’é…ç½®æ“ä½œæ•°",
                        timestamp=time.time()
                    )
                ]
            }
        
        result = benchmark.run_benchmark(config_ops_test)
        
        # é…ç½®æ“ä½œåº”è¯¥å¾ˆå¿«
        ops_per_second = result.metrics[0].value if result.metrics else 0
        assert ops_per_second > 1000  # æ¯ç§’è‡³å°‘1000æ¬¡æ“ä½œ
        
        return result

class TestOverallSystemPerformance:
    """æ•´ä½“ç³»ç»Ÿæ€§èƒ½æµ‹è¯•"""
    
    def test_end_to_end_performance(self, test_env):
        """ç«¯åˆ°ç«¯æ€§èƒ½æµ‹è¯•"""
        benchmark = PerformanceBenchmark("end_to_end_system", iterations=3)
        
        def e2e_test():
            from unittest.mock import patch, Mock
            
            start_time = time.perf_counter()
            
            # æ¨¡æ‹Ÿå®Œæ•´çš„ç›®å½•ç”Ÿæˆæµç¨‹
            with patch('utils.recipes.load_data') as mock_load_data, \
                 patch('utils.recipes.prepare_template') as mock_prepare_template, \
                 patch('utils.recipes.generate_one_archive_directory') as mock_generate, \
                 patch('utils.recipes.get_subset') as mock_get_subset, \
                 patch('utils.recipes.cleanup_stream') as mock_cleanup, \
                 patch('utils.recipes.xw.App') as mock_app:
                
                from utils.recipes import create_jn_or_jh_index
                
                # è®¾ç½®æ¨¡æ‹Ÿæ•°æ®
                mock_data = create_mock_archive_data(50)
                mock_load_data.return_value = mock_data
                mock_prepare_template.return_value = Mock()
                mock_get_subset.return_value = mock_data['æ¡ˆå·æ¡£å·'].unique()
                mock_generate.return_value = 2
                
                # è®¾ç½®xlwingsæ¨¡æ‹Ÿ
                mock_app_instance = Mock()
                mock_wb = Mock()
                mock_sheet = Mock()
                mock_range = Mock()
                
                mock_app.return_value = mock_app_instance
                mock_app_instance.books.open.return_value = mock_wb
                mock_wb.sheets = [mock_sheet]
                mock_sheet.range.return_value = mock_range
                mock_wb.close = Mock()
                mock_app_instance.quit = Mock()
                
                # æ‰§è¡Œå®Œæ•´æµç¨‹
                create_jn_or_jh_index(
                    catalog_path='test_catalog.xlsx',
                    template_path='test_template.xlsx',
                    output_folder=test_env.temp_dir,
                    recipe_name='å·å†…ç›®å½•'
                )
                
                e2e_time = time.perf_counter() - start_time
                
                return {
                    'e2e_time': e2e_time,
                    'archives_processed': len(mock_data['æ¡ˆå·æ¡£å·'].unique()),
                    'metrics': [
                        PerformanceMetric(
                            name="archives_per_second",
                            value=len(mock_data['æ¡ˆå·æ¡£å·'].unique()) / e2e_time,
                            unit="archives/s",
                            description="æ¯ç§’å¤„ç†æ¡£æ¡ˆæ•°",
                            timestamp=time.time()
                        )
                    ]
                }
        
        result = benchmark.run_benchmark(e2e_test)
        
        # ç«¯åˆ°ç«¯å¤„ç†åº”è¯¥åœ¨åˆç†æ—¶é—´å†…å®Œæˆ
        assert result.avg_time < 10.0  # åº”è¯¥åœ¨10ç§’å†…å®Œæˆ
        
        # ä¿å­˜æ€§èƒ½åŸºçº¿
        benchmark.save_baseline()
        
        return result

if __name__ == "__main__":
    # è¿è¡Œæ‰€æœ‰æ€§èƒ½æµ‹è¯•
    pytest.main([
        __file__,
        "-v",
        "--tb=short",
        "-x"  # é‡åˆ°ç¬¬ä¸€ä¸ªå¤±è´¥å°±åœæ­¢
    ])