# 技术架构

## Technical Constraints and Integration Requirements (Revised)

### Maintain Existing Technology Stack

**Current Technology Choices**:

| Category | Technology | Version | Usage Notes |
|---|---|---|---|
| Runtime | Python | 3.x | Main development language |
| GUI Framework | Tkinter | Built-in | Desktop interface, supports real-time logging |
| Excel Ops | xlwings | 0.28.x-0.31.x | Excel automation, depends on Office |
| | openpyxl | 3.0.x-3.1.x | Excel read/write, standalone library |
| | pywin32 | 305+ | Windows COM automation |
| Image Proc | Pillow | 9.0.x-10.x | Font measurement and image processing |
| Data Proc | pandas | 1.5.x-2.3.x | Data analysis and processing |
| System Int | win32print/win32gui | pywin32 | Precise measurement via Windows GDI API |

### New Technology Stack (Redline Revision)

**Technology Selection Matrix**:

| Category | Technology Choice | Purpose | Rationale |
|---|---|---|---|
| Workflow Engine| Self-developed SQLite DAG Engine | Node scheduling & execution | Lightweight (50MB), fully controllable, no external dependencies |
| LLM Strategy | **Hunyuan-1.8B/4B(INT4)** or **Qwen3-4B-Instruct(INT4)** | Resident default | Hard limits: LLM peak ≤4GB, AI concurrency ≤1 |
| | 7B(INT4) on-demand loading | Called on low confidence | Auto-unload when idle to free resources |
| OCR Dual-Channel| **UMI-OCR(HTTP)** or **Embedded PaddleOCR(+OpenVINO)** | Default channel | Embedded preferred for batch/backpressure to improve throughput |
| | **dots.ocr** | Advanced channel | Triggered by complex layouts/formulas/mixed languages |
| Message Queue | SQLite WAL Mode | Inter-node communication | `journal_mode=WAL` + short transactions <50ms |
| Config Mgmt | **Existing ConfigManager + JSON Schema** | Foundation for global config | Avoids dual systems, maintains consistency |
| PDF Processing| PyMuPDF (fitz) | Document processing | Excellent performance, low memory footprint, supports dual-layer PDF |

**Technology Selection Matrix (Redline Revision)**:

| Category | Default/Recommended | Optional/On-demand | Not Adopted & Why |
|---|---|---|---|
| LLM | **Hunyuan-1.8B/4B(INT4)** / **Qwen3-4B-Instruct(INT4)** | 7B(INT4) on-demand | Resident 7B: Squeezes resources on 8GB machines (on-demand only) |
| OCR | **UMI-OCR(HTTP)**, **Embedded PaddleOCR(+OpenVINO)** | **dots.ocr** (for complex layouts) | GPU/VLM routes are "advanced channels" only, not default |
| Workflow/Queue| **Self-developed SQLite DAG + WAL** | — | Airflow/Redis: Too heavyweight, violates "zero external dependency" rule |
| Config/Validation| **Use existing JSON Schema (ConfigManager)** | Dynaconf for env setup only| Avoid dual systems (Cerberus not for global foundation) |
| PDF | **PyMuPDF** | — | — |

### Integration Architecture Design

**Atomic Node Categories**:

1.  **Data I/O Nodes**:
    -   FileInputNode, DatabaseInputNode, APIInputNode
    -   FileOutputNode, DatabaseOutputNode, APIOutputNode

2.  **Intelligent Processing Nodes**:
    -   UMIOCRNode (HTTP interface), LLMProcessingNode（默认 1.8B/4B，7B 按需）
    -   MetadataExtractionNode, QualityCheckNode

3.  **Traditional Processing Nodes**:
    -   RuleBasedValidationNode, FileOperationNode
    -   DataTransformationNode, FormatConversionNode

4.  **Automation Control Nodes**:
    -   FileWatcherNode, ScheduleNode, ConditionNode
    -   ManualReviewNode, ApprovalNode

### Migration and Rollback SOP (Actionable)

**Feature Flag Control**:
```yaml
feature_flags for canary release:
  enable_node_engine: false      # Use legacy version by default
  enable_ai_nodes: false         # Use rule-based by default
  enable_umi_ocr: false          # Use Pillow scheme by default
  migration_mode: "shadow"       # Run in parallel, no impact on users

shadow_write_comparison:
  new_old_parallel: true        # Run new and old systems simultaneously
  comparison_threshold: 0.05    # Switch only if discrepancy rate < 5%
  logging_enabled: true         # Retain full comparison logs
  
rollback_triggers:
  performance_degradation: 0.20   # Performance drops by > 20%
  error_rate_threshold: 0.02     # Error rate exceeds 2%
  user_feedback_severity: "High" # High-severity user feedback

4_step_standard_rollback_procedure:
  1. Flip Switches: Immediately switch feature flags back to legacy version.
  2. Cleanup Temp: Purge temporary data generated by the new system.
  3. Restore Old Flow: Resume the original workflow.
  4. Log Root Cause: Document the reason for rollback with data analysis.
```

### Workflow Versioning and Replayability

**Versioning Standards**:
```yaml
workflow_metadata:
  schema_version: "1.0"           # Workflow definition version
  workflow_hash: "sha256..."      # SHA256 of the configuration file
  created_by: "user_id"           # Creator
  approved_by: "manager_id"       # Approver
  effective_date: "2025-01-01"    # Effective date
  rollback_version: "0.9"         # Target version for rollback
  immutable_archive: true         # Immutable archiving

replay_binding_requirements:
  input_snapshot: "sha256..."     # Input data snapshot
  environment_fingerprint: "..."  # Environment fingerprint
  reproducible_validation: true   # Ensure results can be re-verified

audit_event_dictionary_and_export_signature:
  WORKFLOW_CREATE: Workflow created
  NODE_EXECUTE: Node executed
  DATA_INPUT: Data input
  DATA_OUTPUT: Data output
  ERROR_OCCURRED: Error occurred
  USER_INTERVENTION: Manual intervention
  CONFIG_CHANGE: Configuration changed
  ROLLBACK_EXECUTED: Rollback executed

anti_tampering_via_event_hash_chain:
  - Event hash chain (prev_hash)
  - Daily root hash signature
  - Export to CSV/JSONL + signed manifest
  - Retention period ≥ 3 years
  - Dual approval and privilege escalation alerts for high-risk admin operations
```

## Node Architecture System Analysis (Revised)

### Performance Feasibility Validation (Revised based on research)

**Performance Benchmarks under Hardware Constraints**:
- Lightweight Node Concurrent Execution: Total memory footprint ~400MB (5 nodes).
- LLM Node (Qwen-7B-Chat-Int4): Memory footprint ~4GB, inference speed 50+ tokens/s.
- OCR Node (UMI-OCR): Called via HTTP, local memory footprint < 200MB.
- Remaining System Resources: 3.6GB, sufficient for concurrency and caching needs.

**Actual Performance Metrics** (Based on research data):
- Lightweight Nodes: Process 1000 records in 5-10 seconds.
- AI Nodes: Process 10 texts in 30-60 seconds; process 50 images in 60-120 seconds.
- Maximum Safe Concurrency: 1 AI node + 3 lightweight nodes.

### Technology Selection Validation Results (Based on research)

**UMI-OCR Integration Validation**:
- ✅ Complete HTTP API, supports base64 input and multiple output formats.
- ✅ Native support for dual-layer PDF, perfectly matching archive standards.
- ✅ Based on PaddleOCR v3, high recognition accuracy, simple deployment.
- ✅ "Green" version (unpack and run), no complex dependencies.

**Qwen-7B Model Validation**:
- ✅ Int4 quantized version has a 4GB memory footprint, fitting within the budget.
- ✅ C-Eval Chinese benchmark score of 59.7%, strong capability in handling archival text.
- ✅ Inference speed of 50+ tokens/s, providing an acceptable user experience.
- ✅ Maintained by Alibaba DAMO Academy, active community, timely updates.

**Self-developed DAG Engine Validation**:
- ✅ Based on SQLite WAL mode, supports concurrent reads/writes.
- ✅ Memory footprint of 50MB, 10x lighter than Airflow (500MB+).
- ✅ Task latency of 10-50ms, 5-10x faster than enterprise-grade engines.
- ✅ Fully controllable, optimized for archival scenarios.

### Development Priority Ranking (Revised based on risk assessment)

**Phase 1: Infrastructure Construction** (3 months, low risk, high value)
1.  **Self-developed DAG Engine** - 50MB memory footprint, optimized for archives.
2.  **Feature Flag System** - Support safe migration and rapid rollback.
3.  **Rule-based Validation Node** - Based on Cerberus, immediate data quality improvement.
4.  **File Processing Node** - Reuse existing code for stability and reliability.

**Phase 2: Intelligent Capability Integration** (2 months, medium risk, high value)
5.  **UMI-OCR Integration** - Via HTTP API call, low technical risk.
6.  **Qwen-7B Deployment** - Quantized model, controllable memory.
7.  **Intelligent Validation Node** - Hybrid of rules and AI, accuracy 95%+.

**Phase 3: Advanced Features** (2 months, medium risk, medium value)
8.  **Visual Editor** - Enhance user experience.
9.  **API Standardization** - Support integration with external systems.
10. **Monitoring and Auditing** - Meet compliance requirements.

### Acceptance Criteria (Quantifiable and Testable)

**Node-Level Acceptance Criteria**:
```yaml
general_node_standards:
  interface_spec: Clear I/O types, complete error semantics.
  performance_req: Memory ≤ 200MB, 1000 records ≤ 3-10s (node-dependent), error rate < 1%.
  quality_assurance: Unit test coverage ≥ 80%, integration tests 100% passed.

ai_ocr_node_standards:
  throughput_baseline: Provide dual baselines for UMI-OCR and Paddle(OV) (pages/min).
  accuracy_req: Field-level F1 score ≥ 85%; for complex layouts (dots.ocr), set thresholds for table TEDs/formula edit distance.
  resource_control: Peak memory ≤ 2GB; uncertainty must trigger fallback/manual review path.

ai_llm_node_standards:
  performance_metrics: P95 latency (batch size B=10) ≤ 60s; field-level F1 score ≥ 90%.
  resource_control: Peak memory ≤ 4GB; AI concurrency ≤ 1; 100% success rate for idle unloading.

system_level_acceptance_criteria:
  workflow_support: Max 50 nodes, AI concurrency ≤ 1 (Hard Limit).
  stability_req: No resource leaks (Excel/COM/GDI handles, memory) during a ≥4-hour long-run test.
  user_experience: When queuing/backpressure is triggered, GUI input echo < 200ms, main thread must not be blocked.
  backpressure_drill: Backpressure and protection mode drills must pass.
```

### Risk Control Measures (Reinforced)

**Technical Risk Mitigation**:
```yaml
architecture_complexity_risk:
  mitigation: Strict node interface specifications, comprehensive debug logging.
  monitoring: Node dependency graph visualization, circular dependency detection.

performance_bottleneck_risk:
  mitigation: Layered scheduler, intelligent resource management, degradation strategies.
  monitoring: Real-time memory monitoring, queue length alerts.

learning_curve_risk:
  mitigation: Pre-set workflow templates, wizard-style configuration, thorough documentation.
  monitoring: User path analysis, pain point collection.

data_security_risk:
  mitigation: End-to-end encryption, local processing, complete auditing.
  monitoring: Access log auditing, anomaly behavior detection.
```

## Technical Implementation Architecture (Optimized based on research)

### Core Technical Components

```python
class OptimizedNodeEngine:
    """Node execution engine optimized based on research"""
  
    def __init__(self):
        # Use SQLite WAL mode to avoid read/write blocking
        self.state_db = SQLiteStateManager(wal_mode=True)
      
        # Memory-aware scheduler, triggers backpressure at 6.5GB
        self.scheduler = MemoryAwareScheduler(
            memory_limit_mb=6656,  # 6.5GB
            ai_concurrency=1,
            regular_concurrency=3
        )
      
        # UMI-OCR HTTP integration
        self.ocr_node = UMIOCRNode(
            api_url="http://127.0.0.1:1224/api/ocr",
            fallback=PaddleOCRNode()
        )
      
        # Local deployment of Qwen-7B
        self.llm_node = QwenLLMNode(
            model_path="Qwen-7B-Chat-Int4",
            max_memory_mb=4096
        )
```

### Key Technical Features

**1. Memory Management Optimization**:
```python
class MemoryOptimizer:
    def __init__(self):
        self.memory_monitor = MemoryMonitor()
        self.gc_scheduler = GCScheduler()
      
    def check_memory_pressure(self):
        current_usage = self.memory_monitor.get_usage_mb()
        if current_usage > 6656:  # 6.5GB threshold
            self.trigger_backpressure()
            self.gc_scheduler.force_cleanup()
```

**2. SQLite/WAL Operational Guidelines**:
```python
class OptimizedSQLiteQueue:
    def __init__(self):
        # WAL mode to avoid read/write blocking
        self.conn.execute("PRAGMA journal_mode=WAL")
        # Optimize synchronous mode for better performance
        self.conn.execute("PRAGMA synchronous=NORMAL")
        # Set busy timeout
        self.conn.execute("PRAGMA busy_timeout=5000")
        # Set an appropriate cache size
        self.conn.execute("PRAGMA cache_size=10000")
      
    def execute_short_transaction(self, sql, params):
        """Short transactions <50ms, batch commit ≤500 items"""
        # Composite index on (status, created_at)
        # Retain data for ≤7 days + daily VACUUM
        # 3 exponential backoff retries -> dead-letter table
        pass
```

**3. UMI-OCR Integration**:
```python
class UMIOCRNode(ProcessingNode):
    def __init__(self):
        self.api_url = "http://127.0.0.1:1224/api/ocr"
        self.backup_ocr = PaddleOCRNode()
        self.timeout_config = {
            "connect_timeout": 30,
            "read_timeout": 120,
            "retry_attempts": 3,
            "backoff_factor": 2.0  # Exponential backoff
        }
      
    def process_to_pdf(self, input_data):
        """Enhanced I/O path with file path priority and robust retry mechanism"""
        try:
            # Priority 1: File path/chunked upload (preferred for bandwidth efficiency)
            if hasattr(input_data, 'file_path') and input_data.file_path:
                return self._process_via_file_path(input_data.file_path)
            
            # Priority 2: Base64 fallback (for compatibility)
            elif hasattr(input_data, 'base64_data') and input_data.base64_data:
                return self._process_via_base64(input_data.base64_data)
            
            else:
                raise ValueError("Neither file_path nor base64_data provided")
                
        except Exception as e:
            # Fallback to embedded PaddleOCR
            return self.backup_ocr.process(input_data)
    
    def _process_via_file_path(self, file_path):
        """File path upload with chunked support and resumption"""
        for attempt in range(self.timeout_config["retry_attempts"]):
            try:
                # Chunked upload for large files (>10MB)
                if os.path.getsize(file_path) > 10 * 1024 * 1024:
                    return self._chunked_upload_process(file_path)
                else:
                    # Direct file upload
                    with open(file_path, 'rb') as f:
                        files = {'file': f}
                        response = requests.post(
                            f"{self.api_url}/file",
                            files=files,
                            timeout=(self.timeout_config["connect_timeout"], 
                                   self.timeout_config["read_timeout"])
                        )
                    return self._parse_response(response)
                    
            except (requests.Timeout, requests.ConnectionError) as e:
                if attempt < self.timeout_config["retry_attempts"] - 1:
                    wait_time = self.timeout_config["backoff_factor"] ** attempt
                    time.sleep(wait_time)  # Exponential backoff
                    continue
                raise
    
    def _process_via_base64(self, image_base64):
        """Base64 fallback with timeout and idempotency"""
        payload = {
            "base64": image_base64,
            "options": {
                "data.format": "dict",
                "tbpu.parser": "multi_para"
            },
            "request_id": f"ocr_{int(time.time())}_{hash(image_base64[:100])}"  # Idempotency key
        }
        
        for attempt in range(self.timeout_config["retry_attempts"]):
            try:
                response = requests.post(
                    self.api_url,
                    json=payload,
                    timeout=(self.timeout_config["connect_timeout"], 
                           self.timeout_config["read_timeout"]),
                    headers={"Content-Type": "application/json"}
                )
                return self._parse_response(response)
                
            except (requests.Timeout, requests.ConnectionError) as e:
                if attempt < self.timeout_config["retry_attempts"] - 1:
                    wait_time = self.timeout_config["backoff_factor"] ** attempt
                    time.sleep(wait_time)
                    continue
                raise
    
    def _chunked_upload_process(self, file_path):
        """Chunked upload with breakpoint resumption support"""
        chunk_size = 5 * 1024 * 1024  # 5MB chunks
        file_size = os.path.getsize(file_path)
        upload_id = f"upload_{int(time.time())}_{hash(file_path)}"
        
        # Implementation would include chunk upload logic with resumption
        # and progress tracking for large archive image files
        pass
        
    def _parse_response(self, response):
        """Parse OCR response with error handling"""
        if response.status_code == 200:
            return response.json()
        elif response.status_code == 429:  # Rate limit
            raise requests.exceptions.RetryError("Rate limit exceeded")
        else:
            response.raise_for_status()
```

